.. highlight:: bash

===========================================
Geomicro Illumina Reads processing Pipeline
===========================================
:Online version: available as `google doc`_
:Maintainer: Robert <heinro@umich.edu>

Prerequisites
=============

    #. Have a user account on flux (with allocation), vondamm, or cayman
    #. Have access to long-term data storage
    #. Log into a terminal session on one of the servers
    #. Have the :doc:`omics <omics>` module loaded


Illumina Data
=============

Importing the raw sequence data:
""""""""""""""""""""""""""""""""
Data will generally be available from the UMich Sequencing core via FTP. In
order to download the data, first make sure you have sufficient disk space. As
of October 2015, one lane of Illumina HiSeq 2500 data takes about 45GB. Next,
use the FTP protocol to download this data. Here is the command that has
worked for me::

    wget -m --ask-password --user=PI-LOGIN ftp://ruddle.brcf.med.umich.edu/Run_####

The raw data should be stored in a permanent place and be read-only.  You may
want to write-protect it with a command like::

    chmod -w *.fastq.gz

to protect against accidental deletion.  Consider making a separate backup
copy on a different storage medium, possibly off-line.

What does it look like?
""""""""""""""""""""""""""
The raw data is a collection of :command:`gzip`-compressed FASTQ files.  Sometimes,
rather than providing us one FASTQ giant file for each read direction, the
reads are chopped into FASTQ files containing 4 million reads each. The file
names looked like this:

    :file:`9068_NoIndex_L007_R1_008.fastq.gz`

In this example, 9068 identifies the sample, L007 refers to the 7th lane, R1
means forward directionality (R2 is reverse), and 008 means this is the 8th
file in the series of chopped FASTQ files with forward directionality.

The Pipeline
============

Raw sequencing data should be analyzed and transformed before it can be
applied in a scientific context. The following pipeline will take you through
the steps involved in transforming the data:

    Steps:
	0. Preparation
	1. Quality Control (QC)
	2. Assembly
	3. Mapping
	4. Binning
	5. Bin QC and Chimera Removal

Preparation
===========

Input data for the pipeline and intermediate files should be stored temporarly
in a fast, sufficiently large storage area.  This is not necessarily where you
keep your raw data.  It is recommended to create and use a work directory such
as :file:`/fast/{username}` on vondamm and cayman and
:file:`/scratch/{flux_allocation}/{username}` on flux.  These directories
should be cleaned up after use so other users have the space available.

The QC step assumes the raw data to be available in uncompressed FASTQ format,
one file per read direction and each sample's data in its own directory.  The
:program:`prep` script will decompress and (if needed) concatenate split fastq
files and put them into appropriately named directories::

    cd <work directory>
    prep <raw data directory>

To make use of multiple CPUs replace the simple call to :program:`prep` with a line like::

    for i in {0..9}; do prep /path/to/raw_reads/661${i}*.fastq & done

to distribute the work across you available CPUs (10 in this example) that
distributes processing samples whose filenames start with the same four digits
to the same CPU.

Quality control module - :program:`qc`
======================================

To ensure a computationally manageable assembly at Van acceptable level of
quality the pipeline detailed below can be used.

1.        Initial quality assessment.
"""""""""""""""""""""""""""""""""""""
Before improving the quality it is important to see how good/bad the reads
were to begin with. `FastQC`_ is a tool that automates this step. the pipeline
runs this process in the background twice, once before making any changes to
the reads file and once in the end to see if there was any improvement in the
overall quality of the data.

2.        Dereplicate Reads
"""""""""""""""""""""""""""
After concatenating the fastq files, we dereplicate the fastq to compress
the size of the file. We do this using the :program:`dereplicate` script. This
script goes through the fastq file and clusters together identical sequences
such that all sequences that are 100% identical over 100% lengths form 1
cluster. The script also makes the sequence in a cluster that has the highest
quality score the 'representative' for the cluster.

Output:


The script :program:`dereplicate` will create 2 files as its output, the
".clust" and the ".clust.list". These files are explained below.

:The ".clust" file:
    This file contains the cluster number, cluster size and the names of all
    sequences in that particular cluster. The third column in the file is the
    representative for each cluster. The file is tab-delimited.

:The ".clust.list" file:
    This file contains a list of all representative sequences in the fastq
    file. The file is tab-delimited.

3.        Adapter Removal
"""""""""""""""""""""""""
Adapters used at the library prep stage often make their way into the final
reads. Since these read fragments are not real biological signals, it becomes
important to screen them out. This is where adapter removal comes handy. If
you know the adapters that were used, you can provide an adapter file and tell
the :program:`qc` script to use it.  Otherwise a standand adapter file that is
shipped by the `Trimmomatic`_ tool is used.  The `Scythe`_ tool is used to
screen for adapters.

4.        Trim the Data
"""""""""""""""""""""""
Illumina reads often contain regions of low quality which can negatively
impact assembly. `Sickle`_ is an adaptive read trimmer which uses a windowed
approach to remove regions of low quality from all reads.

5.        Final Quality Assessment
""""""""""""""""""""""""""""""""""
Now that we’ve removed all the data with questionable quality from our
dataset, we need to measure how much (if any) of an improvement was made. So,
we run FastQC again in the background while we move on to the next steps.

6.        Interleaving (Assembler Dependent)
""""""""""""""""""""""""""""""""""""""""""""
This is an optional, assembler dependent step. Current assembly pipeline
requires an interleaved FASTA file. This portion of the pipeline simply pairs
a forward and its corresponding reverse sequence such that they occur one
after the other in a single file. In other words, if the nth sequence is
forward then its reverse counterpart should be the (n+1)th sequence.

Running the QC Module
"""""""""""""""""""""

Assuming :program:`prep` was run as above, the :program:`qc` script is run for
each sample::

    cd <working directory>/<sample id>
    qc

You can easily process multiple samples in parallel by doing something like::

    cd <working directory>
    for i in *; do (cd $i && qc &); done

The :program:`qc` script makes use of two CPUs in parallel so the above works
best when the number of CPUs is twice the number of samples.  If there are more
samples multiple `for` loops can be used with the globs (`*`) replaced with an
apropriate number of sample ids.

Assembly Module -- :program:`assemble`
======================================

Assemblies can take a lot of computational time and resources. This script was
written such that once you have QC-ed all your samples, you may run this
module once and it’ll step through each of your Samples assembling them
individually along the way. This module ensures that your assembles will run
with the maximum possible resources and with least possible downtime between
assemblies.


1.        Assemble
""""""""""""""""""
As of October 2015, `IDBA_UD`_ remains the assembler of choice. Once the data is
interleaved, IDBA uses just the interleaved reads for assembly. One merit of
using IDBA_UD over other assemblers such as velvet is that, you may provide
IDBA with a range of *k*-mer sizes (default: 58 <= *k* <= 92) and a step size
*s* (default: *s* = 8). This will make IDBA
assemble your data using 52-mers, increase the *k* by the step size
(52+8=60) and assemble the data again for the next *k*-mer (60). Once IDBA has
cycled through each *k* it combines all the assemblies into one
super assembly. Although, IDBA produces a number of output files, we use
:file:`scaffolds.fa` as the final output for this step.


2.        Quality Check
"""""""""""""""""""""""
Once we’ve assembled our reads into scaffolds, we need to see how well we did?
This is where `QUAST`_ comes in. Quast reads the assembled sequences and
creates a report depicting how good the assembly was. It calculates the N50
(higher is better), the L50 (lower is better), number of unique genes and
produces some interactive plots as well.


3.        Silva Blast
"""""""""""""""""""""
One of the more important reasons why we prefer de novo assembly over read
mapping to references is that, the assembly gives us an opportunity to find
novel organisms. In order to see if what we’ve assembled, we can compare our
sequences to a curated database like the `SILVA`_ SSU using BlastN. This shows us
in a quantitative manner 16S genes that might be novel or simply may be of
interest. Another reason for such blasts is to select references genomes for
ESOM. So we can seed the ESOM with known organisms thus helping in the binning
of the unknowns.


4.        NCBI Blast
""""""""""""""""""""
Like searching SILVA above but with a more expansive NCBI
database.


5.        Phylosift
"""""""""""""""""""
Generates a taxonomic profile of the metagenome based not just on the 16S data
(like SILVA or NCBI Blasts) but using a curated marker set database. This is
helpful especially because 16S regions can be notoriously difficult to
assemble. Using `Phylosift`_ allows us to look at the phylogenetic distribution
of the metagenome even if a 16S was not assembled.

Running the Assembly Module:
""""""""""""""""""""""""""""

You may assemble each sample individually or assemble some or all of your samples
together.  To assemble the reads of a single sample do::

    cd <working directory>/<sample id>
    assemble

To assemble all your samples do something like this::

    cd <working directory>
    assemble *

The glob :samp:`*` will expand to a list of all sample directories i.e. the sample
ids, as made with the :program:`prep` script.  You can replace the ``*`` with
a subset of sample ids that shall go into the assembly.


Mapping
=======

After performing an assembly, we can align the reads used to the create the
assembly with the contigs produced by the assembly. This allows us to calculate
how many reads recruit to each contig.  This preparation for the binning step
but also allows visualization via :program:`igv`.

Mapping has to occur seperately for each sample and before the actual mapping
the assembly needs to be indexed.  This indexing only needs to be performed
once.  Also, in preparation to do binning with CONCOCT, the assembled contigs
should be chopped-up into equal-sized chunks before doing any of these.  So
assume the previous steps of the pipeline were followed there should be an
assembly :file:`assembly.fa` and per-sample directories containing the reads
for each sample.  Do this::

    chop-contigs -i assembly.fa -o assembly.chop.fa
    # TODO: how to do separate indexing
    for i in sample_*; do
        cd $i
        mapping --no-indexing --assembly assembly.chop.fa
        cd ..
    done


Binning
=======

Do this::

    binning --assembly assembly.chop.fa Sample_*


.. _google doc: https://docs.google.com/a/umich.edu/document/d/1z0C27ECGM2CCrk6pHwQs5VFwchatrStmiyjCzTqtUM0/edit?usp=sharing
.. _FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/
.. _IDBA_UD: https://i.cs.hku.hk/~alse/hkubrg/projects/idba_ud/
.. _Phylosift: https://phylosift.wordpress.com/
.. _QUAST: http://quast.sourceforge.net/
.. _Scythe: https://github.com/vsbuffalo/scythe
.. _Sickle: https://github.com/najoshi/sickle
.. _SILVA: https://www.arb-silva.de/
.. _Trimmomatic: http://www.usadellab.org/cms/index.php?page=trimmomatic
