.. highlight:: bash

===========================================
Geomicro Illumina Reads processing Pipeline
===========================================
:Online version: available as `google doc`_
:Maintainer: Robert <heinro@umich.edu>

Preface
-------

These notes document the best-practices of the University of Michigan
Geomicrobiology group for processing meta-genomic sequencing data.  It is
assumed that that reader has DNA sequencing data obtained from the Illumina
HiSeq platform.  This document is intended to be descriptive and subject to
ongoing revision.  Contributions and comments should be directed at the
`maintainer`_.  This is a guide for newcomers as well as a reference for the
more experienced.

Other sources of information
----------------------------

    * The Bioinformatics Tutorial at the `Geomicrobiology Google Drive`_.
    * The `um-microbiomics forum`_.


Prerequisites
-------------

    #. Have a user account on FLUX (with high-mem allocation), or the Geomicrobiology servers
    #. Have access to long-term data storage
    #. Log into a terminal session on one of the servers

Working on FLUX
---------------

Environment setup
""""""""""""""""""
On FLUX, running this in your terminal's shell will set up your environment::

    $ module use /dept/geology/geomicro/data9/flux/modulefiles
    $ module load geomicro/omics

This setup can be saved for subsequent logins by::

    $ module save

Loading the ``omics`` module will also load several additional modules in the
background.  Usually it is okay to load modules as needed and save the setup as
above.  In rare cases modules can interact strangely in ways that might be
difficult to debug.  Refer to the documentation of the :command:`module`
command, e.g. type ``module help``.

The ``omics`` container
"""""""""""""""""""""""

To provide a more consistent, reproducible, setup for data analysis a
`Singularity container`_ is provided.  This provides a standard linux
environment (Debian stable) tailored for the pipeline but isolated from FLUX.
Inside the container none of the software provided by FLUX or any modules is
available, instead software installed in the container is used.  The container
image is located at :file:`/dept/geology/geomicro/data9/flux/omics.img` and
is executable, e.g. running::

    $ /dept/geology/geomicro/data9/flux/omics.img

on the flux command line will show a help screen documenting how the container
can be used directly on FLUX without the need for the ``geomicro/omics``
module.  For convenience, the ``geomicro/omics`` module provides the command
:command:`comics` to access the container.  The specifics are documented
separately for interactive use and non-interactive job-submission in
sub-sections following this one.

Copying the container image, a 3+ GB file to your own computer gives you the
same environment.  Installing ``singularity`` and runnning something like::

    $ singularity shell omics.img

will give you access.  If you have super-user rights you can make
modifications, e.g. update or install software.  You can then copy your
modified image back to flux and use it there.


Interactive
"""""""""""

The pipeline's scripts can be run interactively on login nodes (for limited
testing) or during an interactive FLUX session.  Commands can be run the FLUX
environment, e.g. to print the help screen of the assemble script::

    $ omics assemble -h

The recommended way, however, is to first enter the omics container
environment::

    uniqname@flux-login:~$ comics
    Singularity: Invoking an interactive shell within container...

    (omics) ~$

You will be greeted with a special prompt starting with ``(omics)`` and run
commands as usual::

    (omics) ~$ omics assemble -h

To exit the container type ``CTRL-D`` and you return to the usual FLUX
environment.

Submitting FLUX jobs
""""""""""""""""""""

When running pipeline scripts as part of a submitted FLUX job it is also
recommended to use the ``omics`` container.  In this example PBS script::

    #!/bin/bash
    #PBS -N undergrads_0
    #PBS -M heinro@umich.edu
    #PBS -m abe
    #PBS -V
    #PBS -l walltime=7:00:00:00,nodes=1:ppn=20,mem=500gb
    #PBS -q fluxm
    #PBS -A schmidti_fluxm
    #PBS -l qos=flux
    #PBS -j oe

    cd $PBS_O_WORKDIR
    comics -- omics prep /nfs/turbo/schmidt-lab/heinro/undergrads-2016/raw/66*
    comics -- omics qc 66*
    comics -- omics assemble --cpus 20 66*

we run three commands corresponding to the first three steps of the pipeline.
Each command is prepended by ``comics --`` which excecutes the rest of the
command line inside the container.  To instead run everything in the FLUX
environment you would just remove ``comics --`` from each of the last three
lines.

Working on the Geomicrobiology servers
--------------------------------------

On ``cayman`` and ``vondamm`` the ``omics`` container is located at
:file:`/geomicro/data9/flux/omics.img`.  The command :command:`comics` is
available to conveniently enter the container.  The servers each have 64 CPUs
and 754 GB RAM available.  These resources are shared cooperatively between
users so some judgment is needed when deciding how many threads or CPUs to use
for a command.


Illumina Data
-------------

Importing the raw sequence data
"""""""""""""""""""""""""""""""
Data will generally be available from the UMich Sequencing core via FTP. In
order to download the data, first make sure you have sufficient disk space. One
lane of compressed Illumina HiSeq data may require 80GB. Next, use the FTP
protocol to download this data. Here is the command that has worked for me::

    cd /path/to/storage
    wget -m --ask-password --user=<useraccount> ftp://ruddle.brcf.med.umich.edu/Run_####

The raw data should be stored in a permanent place and be read-only.  You may
want to write-protect it with a command like::

    chmod -w *.fastq.gz

to protect against accidental deletion.  Consider making a separate backup
copy on a separate storage medium, possibly off-line.

What does it look like?
""""""""""""""""""""""""""
The raw data is a collection of :command:`gzip`-compressed FASTQ files.
Sometimes, rather than providing us one FASTQ file for each read direction, the
reads are chopped into FASTQ files containing 4 million reads each. The file
names looked like this:

    :file:`9068_NoIndex_L007_R1_001.fastq.gz`

In this example, ``9068`` identifies the sample, ``L007`` refers to the 7th
lane, ``R1`` means forward directionality (``R2`` is reverse), and ``001``
means this is the 1st file in the series of chopped ``FASTQ`` files with
forward directionality.  These days files are not chopped anymore so the last
part is always ``001``.

The Pipeline
------------

Raw sequencing data should be analyzed and transformed before it can be
applied in a scientific context. The following pipeline will take you through
the steps involved in processing the data:

    Steps:
	0. Preparation
	1. Quality Control (QC)
	2. Assembly
	3. Mapping
	4. Binning

Each step has a corresponding section below.  All code examples below are for
the :command:`bash` shell and shown without command prompt and should work in
the ``omics`` container as well as outside the container on FLUX with the
``geomicro/omics`` module loaded.  The examples command lines only show what's
essential to run the pipeline and are missing some highly-recommended options
like e.g. ``--cpus``.


Preparation
-----------

Input data for the pipeline and intermediate files should be stored temporarly
in a fast, sufficiently large storage area.  This is not necessarily where you
keep your raw data.  On FLUX you would work under
:file:`/scratch/{flux_allocation}/{username}` and move the results somewhere
else when you're done. The Geomicrobiology servers don't have a separate
scratch partition and you should work in your home directory.

The QC step assumes the raw data to be available in uncompressed FASTQ format,
one file per read direction and each sample's data in its own directory.  The
:program:`omics prep` script will decompress and (if needed) concatenate split
fastq files and put them into appropriately named directories::

    cd <work directory>
    omics prep <raw data directory>

Quality control module - :program:`qc`
--------------------------------------

To ensure a computationally manageable assembly at an acceptable level of
quality the pipeline detailed below can be used.

Initial quality assessment
""""""""""""""""""""""""""
Before improving the quality it is important to see how good/bad the reads
were to begin with. `FastQC`_ is a tool that automates this step. the pipeline
runs this process in the background twice, once before making any changes to
the reads file and once in the end to see if there was any improvement in the
overall quality of the data.

Dereplicate Reads
"""""""""""""""""
Due to limits of the Illumina sequencing technology an DNA fragment may be
"replicated" and sequenced multiple times, resulting in a number of identical
reads beyond what one expects by pure change.  This step identifies sets of
identical reads per sample (both forward and reverse reads, respectively, must
match exacly over the whole length) and retains only the one with the highest
average quality score.

Adapter Removal
"""""""""""""""
Adapters used at the library prep stage often make their way into the final
reads. Since these read fragments are not real biological signals, it becomes
important to screen them out. This is where adapter removal comes handy. If you
know the adapters that were used, you can provide an adapter file and tell the
:program:`omics qc` script to use it.  Otherwise a standard adapter file that
is shipped by the `Trimmomatic`_ tool is used.  The `Scythe`_ tool or
Trimmomatic can be used to screen for adapters.

Trim the Data
"""""""""""""
Illumina reads often contain regions of low quality, usually at the beginning
and the end, which can negatively impact assembly. Both, the Trimmomatic tool
and `Sickle`_ are adaptive read trimmers which use a windowed approach to
remove regions of low quality from all reads.

Final Quality Assessment
""""""""""""""""""""""""
Now that we’ve removed all the data with questionable quality from our
dataset, we need to measure how much (if any) of an improvement was made. So,
we run FastQC again in the background while we move on to the next steps.

Interleaving (Assembler Dependent)
""""""""""""""""""""""""""""""""""
This is an optional, assembler dependent step. Current assembly pipeline
requires an interleaved FASTA file. This portion of the pipeline simply pairs
a forward and its corresponding reverse sequence such that they occur one
after the other in a single file. In other words, if the nth sequence is
forward then its reverse counterpart should be the (n+1)th sequence.

Running the QC Module
"""""""""""""""""""""

Assuming :program:`omics-prep` was run as above, the :program:`omics qc` script
is run for each sample::

    cd <working directory>/<sample id>
    omics qc

You can easily process multiple samples in parallel by doing something like::

    cd <working directory>
    omics qc --threads 8 *

For the ``--threads`` parameter a low multiple of two is recommended as per the
following reasoning:  Between the individual QC steps intermediate data is
saved in your working directory.  This means your fastq data (5 - 10 GB per
sample) will be read and written 6 or 7 times (per sample again.)  Usually,
your storage is network-attached, e.g. ``/scratch`` on FLUX is not hosted by
your computing node but accessed via the network, storage on the geomicro
servers is organized likewise.  As such, temporarily storing intermediate
results may easily saturate the network and become the bottleneck for this
pipeline step.  The severity depends on the workload of other users sharing
FLUX or the geomicro servers.  Since the script will always process the two
read directions in parallel it makes sense to allocate a multiple of two
threads for QC.

Assembly Module -- :program:`assemble`
--------------------------------------

Assemblies can take a lot of computational time and resources. This script was
written such that once you have QC-ed all your samples, you may run this
module once and it’ll step through each of your Samples assembling them
individually along the way. This module ensures that your assembles will run
with the maximum possible resources and with least possible downtime between
assemblies.


Assemble
""""""""
To assemble the QC'd reads into contigs the pipeline offers the choice between the 
`IDBA_UD`_ and the `MEGAHIT`_ assemblers. Once the data is
interleaved, IDBA uses just the interleaved reads for assembly. One merit of
using IDBA_UD over other assemblers such as velvet is that, you may provide
IDBA with a range of *k*-mer sizes (default: 58 <= *k* <= 92) and a step size
*s* (default: *s* = 8). This will make IDBA
assemble your data using 52-mers, increase the *k* by the step size
(52+8=60) and assemble the data again for the next *k*-mer (60). Once IDBA has
cycled through each *k* it combines all the assemblies into one
super assembly. Although, IDBA produces a number of output files, we use
:file:`scaffolds.fa` as the final output for this step.


Quality Check
"""""""""""""
Once we’ve assembled our reads into scaffolds, we need to see how well we did?
This is where `QUAST`_ comes in. Quast reads the assembled sequences and
creates a report depicting how good the assembly was. It calculates the N50
(higher is better), the L50 (lower is better), number of unique genes and
produces some interactive plots as well.


Silva Blast
"""""""""""
One of the more important reasons why we prefer de novo assembly over read
mapping to references is that, the assembly gives us an opportunity to find
novel organisms. In order to see if what we’ve assembled, we can compare our
sequences to a curated database like the `SILVA`_ SSU using BlastN. This shows us
in a quantitative manner 16S genes that might be novel or simply may be of
interest. Another reason for such blasts is to select references genomes for
ESOM. So we can seed the ESOM with known organisms thus helping in the binning
of the unknowns.


NCBI Blast
""""""""""
Like searching SILVA above but with a more expansive NCBI
database.


Phylosift
"""""""""
Generates a taxonomic profile of the metagenome based not just on the 16S data
(like SILVA or NCBI Blasts) but using a curated marker set database. This is
helpful especially because 16S regions can be notoriously difficult to
assemble. Using `Phylosift`_ allows us to look at the phylogenetic distribution
of the metagenome even if a 16S was not assembled.

Running the Assembly Module:
""""""""""""""""""""""""""""

You may assemble each sample individually or assemble some or all of your samples
together.  To assemble the reads of a single sample do::

    cd <working directory>/<sample id>
    omics assemble

To assemble all your samples do something like this::

    cd <working directory>
    omics assemble *

The glob :samp:`*` will expand to a list of all sample directories i.e. the
sample ids, as made with the :program:`omics prep` script.  You can replace the
``*`` with a subset of sample ids that shall go into the assembly.


Mapping
-------

After performing an assembly, we can align the reads used to the create the
assembly with the contigs produced by the assembly. This allows us to calculate
how many reads recruit to each contig.  This prepares for the binning step
but also allows visualization via :program:`igv`.

Mapping has to occur separately for each sample and before the actual mapping
the assembly needs to be indexed.  This indexing only needs to be performed
once.  Also, in preparation to do binning with CONCOCT, the assembled contigs
should be chopped-up into equal-sized chunks before doing any of these.  So
assume the previous steps of the pipeline were followed there should be an
assembly :file:`assembly.fa` and per-sample directories containing the reads
for each sample.  The canonical way to use :program:`omics
mapping` is as follows::

    omics mapping --chop --cpus 8 *


Binning
-------

Do this::

    omics binning --assembly assembly.chop.fa Sample_*


.. _google doc: https://docs.google.com/a/umich.edu/document/d/1z0C27ECGM2CCrk6pHwQs5VFwchatrStmiyjCzTqtUM0/edit?usp=sharing
.. _FastQC: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/
.. _IDBA_UD: https://i.cs.hku.hk/~alse/hkubrg/projects/idba_ud/
.. _Phylosift: https://phylosift.wordpress.com/
.. _QUAST: http://quast.sourceforge.net/
.. _Scythe: https://github.com/vsbuffalo/scythe
.. _Sickle: https://github.com/najoshi/sickle
.. _SILVA: https://www.arb-silva.de/
.. _Trimmomatic: http://www.usadellab.org/cms/index.php?page=trimmomatic
.. _Singularity container: http://singularity.lbl.gov/
.. _Geomicrobiology Google Drive: https://drive.google.com/open?id=0B3q4ORuyGjkwfjV0Wjh4c1ZVOWVZem5XU1drNWVkeDMxMGZwcGdtd1BRYnc5Tm90VElhejA
.. _maintainer: mailto:heinro@umich.edu
.. _um-microbiomics forum: https://groups.google.com/forum/#!forum/um-microbiomics
.. _MEGAHIT: https://github.com/voutcn/megahit
